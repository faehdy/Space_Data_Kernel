{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0639ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ac3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grace = pd.read_csv('../../Data/interpolated_lwe_grid_resolution5.csv')\n",
    "df_grace = df_grace.rename(columns={'lon_centroid': 'lon', 'lat_centroid': 'lat', 'lwe_thickness': 'waterstorage'})\n",
    "df_grace['lon'] = df_grace['lon'].apply(lambda x: x - 360 if x > 180 else x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad43ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Output CSV: ../../output/downsampled_mascons_5deg_water_aligned.csv\n",
      "Output Parquet: ../../output/downsampled_mascons_5deg_water_aligned.parquet\n",
      "Input resolution: 5° x 5° (assumed)\n",
      "Output resolution: 5.0° x 5.0°\n",
      "\n",
      "Using pre-loaded DataFrame...\n",
      "Converting 'time' column to datetime...\n",
      "DataFrame ready.\n",
      "DataFrame head:\n",
      "        time     lon    lat  waterstorage\n",
      "0 2002-04-01 -127.25  47.75      0.000000\n",
      "1 2002-04-01 -122.25  47.75      2.780771\n",
      "2 2002-04-01 -117.25  47.75      7.942758\n",
      "3 2002-04-01 -112.25  47.75      7.106880\n",
      "4 2002-04-01 -107.25  47.75      4.373613\n",
      "\n",
      "Converting DataFrame to Xarray Dataset...\n",
      "Xarray Dataset created successfully:\n",
      "<xarray.Dataset> Size: 176kB\n",
      "Dimensions:       (time: 272, lat: 5, lon: 16)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 2kB 2002-04-01 2002-05-01 ... 2024-11-01\n",
      "  * lat           (lat) float64 40B 47.75 52.75 57.75 62.75 67.75\n",
      "  * lon           (lon) float64 128B -127.2 -122.2 -117.2 ... -57.25 -52.25\n",
      "Data variables:\n",
      "    waterstorage  (time, lat, lon) float64 174kB 0.0 2.781 ... -1.166e-18 -83.61\n",
      "\n",
      "Aggregating data to 5.0° x 5.0° grid using groupby_bins...\n",
      "Latitude bins: [45. 50. 55. 60. 65. 70.]\n",
      "Latitude centroids: [47.5 52.5 57.5 62.5 67.5]\n",
      "Longitude bins: [-130. -125. -120. -115. -110. -105. -100.  -95.  -90.  -85.  -80.  -75.\n",
      "  -70.  -65.  -60.  -55.  -50.]\n",
      "Longitude centroids: [-127.5 -122.5 -117.5 -112.5 -107.5 -102.5  -97.5  -92.5  -87.5  -82.5\n",
      "  -77.5  -72.5  -67.5  -62.5  -57.5  -52.5]\n",
      "Aggregation complete.\n",
      "Aggregated Dataset:\n",
      "<xarray.Dataset> Size: 176kB\n",
      "Dimensions:       (lon: 16, lat: 5, time: 272)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 2kB 2002-04-01 2002-05-01 ... 2024-11-01\n",
      "  * lat           (lat) float64 40B 47.5 52.5 57.5 62.5 67.5\n",
      "  * lon           (lon) float64 128B -127.5 -122.5 -117.5 ... -62.5 -57.5 -52.5\n",
      "Data variables:\n",
      "    waterstorage  (lon, lat, time) float64 174kB 0.0 4.337e-19 ... -79.62 -83.61\n",
      "\n",
      "Converting aggregated Dataset back to DataFrame...\n",
      "\n",
      "Saving aggregated data...\n",
      "Saving to CSV: ../../output/downsampled_mascons_5deg_water_aligned.csv\n",
      "Saving to Parquet: ../../output/downsampled_mascons_5deg_water_aligned.parquet\n",
      "Aggregated data saved successfully.\n",
      "\n",
      "Final DataFrame snippet:\n",
      "     lon   lat       time  waterstorage\n",
      "0 -127.5  47.5 2002-04-01  0.000000e+00\n",
      "1 -127.5  47.5 2002-05-01  4.336809e-19\n",
      "2 -127.5  47.5 2002-06-01 -2.602085e-18\n",
      "3 -127.5  47.5 2002-07-01 -1.040834e-17\n",
      "4 -127.5  47.5 2002-08-01  3.469447e-18\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os # Added for output directory creation\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assuming df is your loaded DataFrame before this script runs\n",
    "# Example:\n",
    "# df = pd.read_csv('your_input_file.csv')\n",
    "\n",
    "output_dir = '../../output' # Define output directory\n",
    "output_csv_file = os.path.join(output_dir, 'downsampled_mascons_5deg_water_aligned.csv')\n",
    "output_parquet_file = os.path.join(output_dir, 'downsampled_mascons_5deg_water_aligned.parquet')\n",
    "\n",
    "df = df_grace\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the input grid resolution (used for consistency check, not directly in groupby_bins)\n",
    "lat_res_in = 5\n",
    "lon_res_in = 5\n",
    "# Define the desired output grid resolution\n",
    "lat_res_out = 5.0 # Use float for calculations\n",
    "lon_res_out = 5.0 # Use float for calculations\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Starting script...\")\n",
    "print(f\"Output CSV: {output_csv_file}\")\n",
    "print(f\"Output Parquet: {output_parquet_file}\")\n",
    "print(f\"Input resolution: {lat_res_in}° x {lon_res_in}° (assumed)\")\n",
    "print(f\"Output resolution: {lat_res_out}° x {lon_res_out}°\")\n",
    "\n",
    "# --- Step 1: Read the CSV data using Pandas ---\n",
    "# Assume 'df' is already loaded and contains 'time', 'lat', 'lon', 'waterstorage'\n",
    "print(f\"\\nUsing pre-loaded DataFrame...\")\n",
    "try:\n",
    "    # Make sure columns exist before proceeding\n",
    "    required_cols = ['time', 'lat', 'lon', 'waterstorage']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Optional: Convert 'time' column to datetime objects if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        print(\"Converting 'time' column to datetime...\")\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    print(\"DataFrame ready.\")\n",
    "    print(\"DataFrame head:\")\n",
    "    print(df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"Error: DataFrame 'df' not found.\")\n",
    "    print(\"Please ensure the DataFrame is loaded before running this script.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    exit()\n",
    "except MemoryError: # Keep memory error handling\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"MemoryError: The DataFrame is too large.\")\n",
    "    print(\"Consider using Dask or chunking if conversion fails later.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit() # Exit the script if memory error occurs\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn unexpected error occurred during DataFrame preparation: {e}\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Step 2: Convert Pandas DataFrame to Xarray Dataset ---\n",
    "print(\"\\nConverting DataFrame to Xarray Dataset...\")\n",
    "try:\n",
    "    # Check for duplicate indices *before* setting index\n",
    "    duplicates = df.duplicated(subset=['time', 'lat', 'lon']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\nWarning: Found {duplicates} duplicate time/lat/lon combinations. Keeping first occurrence.\")\n",
    "        # Optionally, decide how to handle duplicates (e.g., mean, keep first/last)\n",
    "        # df = df.groupby(['time', 'lat', 'lon']).mean().reset_index() # Example: average duplicates\n",
    "        df = df.drop_duplicates(subset=['time', 'lat', 'lon'], keep='first')\n",
    "\n",
    "    df = df.set_index(['time', 'lat', 'lon'])\n",
    "    ds = df.to_xarray()\n",
    "\n",
    "    # Ensure latitude coordinates are sorted (ascending or descending)\n",
    "    # groupby_bins requires monotonic coordinates\n",
    "    if not ds.indexes['lat'].is_monotonic_increasing and not ds.indexes['lat'].is_monotonic_decreasing:\n",
    "         print(\"Sorting latitude coordinates...\")\n",
    "         ds = ds.sortby('lat')\n",
    "    if not ds.indexes['lon'].is_monotonic_increasing and not ds.indexes['lon'].is_monotonic_decreasing:\n",
    "         print(\"Sorting longitude coordinates...\")\n",
    "         ds = ds.sortby('lon')\n",
    "\n",
    "    # Optional: Reindex latitude to be decreasing if needed (common standard)\n",
    "    # if ds['lat'][0] < ds['lat'][-1]:\n",
    "    #     print(\"Reindexing latitude to decreasing order...\")\n",
    "    #     ds = ds.reindex(lat=list(reversed(ds['lat'])))\n",
    "\n",
    "    print(\"Xarray Dataset created successfully:\")\n",
    "    print(ds)\n",
    "except KeyError as e:\n",
    "    print(f\"\\nError: Column '{e}' not found in DataFrame. Needed for setting index.\")\n",
    "    print(\"Please ensure your DataFrame has 'time', 'lat', 'lon' columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during DataFrame to Xarray conversion: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 3: Perform Spatial Aggregation using groupby_bins ---\n",
    "print(f\"\\nAggregating data to {lat_res_out}° x {lon_res_out}° grid using groupby_bins...\")\n",
    "\n",
    "try:\n",
    "    # 3a: Determine the overall extent and create output grid *boundaries*\n",
    "    min_lat, max_lat = ds['lat'].min().item(), ds['lat'].max().item()\n",
    "    min_lon, max_lon = ds['lon'].min().item(), ds['lon'].max().item()\n",
    "\n",
    "    # Calculate the boundaries aligned with the output resolution\n",
    "    # Use floor for min edge, ceil for max edge, ensuring they align with the grid\n",
    "    # Add a small epsilon to max extent before ceil to handle points exactly on boundary\n",
    "    epsilon = 1e-9\n",
    "    lat_min_bnd = np.floor(min_lat / lat_res_out) * lat_res_out\n",
    "    lat_max_bnd = np.ceil((max_lat + epsilon) / lat_res_out) * lat_res_out\n",
    "    lon_min_bnd = np.floor(min_lon / lon_res_out) * lon_res_out\n",
    "    lon_max_bnd = np.ceil((max_lon + epsilon) / lon_res_out) * lon_res_out\n",
    "\n",
    "    # Create the bin edges for grouping\n",
    "    # Use arange up to max_bnd + resolution/2 to ensure the last bin edge is included\n",
    "    lat_bins = np.arange(lat_min_bnd, lat_max_bnd + lat_res_out / 2, lat_res_out)\n",
    "    lon_bins = np.arange(lon_min_bnd, lon_max_bnd + lon_res_out / 2, lon_res_out)\n",
    "\n",
    "    # 3b: Calculate the desired output grid *centroids* (labels for the bins)\n",
    "    # These will become the new coordinates in the aggregated dataset\n",
    "    lat_centroids = lat_bins[:-1] + lat_res_out / 2.0\n",
    "    lon_centroids = lon_bins[:-1] + lon_res_out / 2.0\n",
    "\n",
    "    print(f\"Latitude bins: {lat_bins}\")\n",
    "    print(f\"Latitude centroids: {lat_centroids}\")\n",
    "    print(f\"Longitude bins: {lon_bins}\")\n",
    "    print(f\"Longitude centroids: {lon_centroids}\")\n",
    "\n",
    "    # 3c: Perform the aggregation\n",
    "    # Group by latitude bins first, calculate the mean for each bin,\n",
    "    # then group the result by longitude bins and calculate the mean.\n",
    "    # Use the calculated centroids as the coordinate labels for the resulting bins.\n",
    "    # 'right=False' means bins are [left, right), matching np.floor logic.\n",
    "    ds_agg = ds.groupby_bins('lat', bins=lat_bins, labels=lat_centroids, right=False).mean()\n",
    "    ds_agg = ds_agg.groupby_bins('lon', bins=lon_bins, labels=lon_centroids, right=False).mean()\n",
    "\n",
    "    # Rename the coordinate dimensions from 'lat_bins'/'lon_bins' back to 'lat'/'lon'\n",
    "    ds_agg = ds_agg.rename({'lat_bins': 'lat', 'lon_bins': 'lon'})\n",
    "\n",
    "    print(\"Aggregation complete.\")\n",
    "    print(\"Aggregated Dataset:\")\n",
    "    print(ds_agg)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during aggregation with groupby_bins: {e}\")\n",
    "    print(\"Check if latitude/longitude coordinates are monotonic (sorted).\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 4: Convert back to DataFrame and save ---\n",
    "print(\"\\nConverting aggregated Dataset back to DataFrame...\")\n",
    "# Use .reset_index() to turn the coordinates ('time', 'lat', 'lon')\n",
    "# back into columns for the CSV/Parquet output.\n",
    "df_agg = ds_agg.to_dataframe().reset_index()\n",
    "\n",
    "# Optional: Drop rows where the aggregated value is NaN\n",
    "# This happens if an output grid cell contained no input data points.\n",
    "nan_count_before = df_agg['waterstorage'].isnull().sum()\n",
    "if nan_count_before > 0:\n",
    "    print(f\"Dropping {nan_count_before} rows with NaN 'waterstorage' values (empty grid cells).\")\n",
    "    df_agg = df_agg.dropna(subset=['waterstorage'])\n",
    "\n",
    "print(f\"\\nSaving aggregated data...\")\n",
    "try:\n",
    "    # Save to CSV\n",
    "    print(f\"Saving to CSV: {output_csv_file}\")\n",
    "    df_agg.to_csv(output_csv_file, index=False, float_format='%.5f') # Control float precision\n",
    "\n",
    "    # Save to Parquet (often more efficient for large data)\n",
    "    print(f\"Saving to Parquet: {output_parquet_file}\")\n",
    "    df_agg.to_parquet(output_parquet_file, index=False)\n",
    "\n",
    "    df_grace = df_agg.copy()\n",
    "\n",
    "    print(\"Aggregated data saved successfully.\")\n",
    "    print(\"\\nFinal DataFrame snippet:\")\n",
    "    print(df_agg.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the output files: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b519c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'lat', 'lon', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst',\n",
      "       'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_gldas = pd.read_csv('../../Data/data_GLDAS/compiled_canada_soil_moisture.csv')\n",
    "print(df_gldas.columns)\n",
    "# df = df.drop(['SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'], axis = 1)\n",
    "\n",
    "\n",
    "df_gldas['waterstorage'] = (df_gldas['SoilMoi0_10cm_inst'] * 10 + df_gldas['SoilMoi10_40cm_inst'] * 30 +df_gldas['SoilMoi40_100cm_inst'] * 60 + df_gldas['SoilMoi100_200cm_inst'] * 100) /200\n",
    "# rename the column 'SoilMoi0_10cm_inst' to 'waterstorage'\n",
    "# df = df.rename(columns={'SoilMoi0_10cm_inst': 'waterstorage'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae49c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Output CSV: ../../output/downsampled_GLDAS_5deg_water_aligned.csv\n",
      "Output Parquet: ../../output/downsampled_GLDAS_5deg_water_aligned.parquet\n",
      "Input resolution: 0.25° x 0.25° (assumed)\n",
      "Output resolution: 5.0° x 5.0°\n",
      "\n",
      "Using pre-loaded DataFrame...\n",
      "Converting 'time' column to datetime...\n",
      "DataFrame ready.\n",
      "DataFrame head:\n",
      "        time     lat      lon  SoilMoi0_10cm_inst  SoilMoi10_40cm_inst  \\\n",
      "0 2000-01-01  82.875  -67.625           32.961132            163.73515   \n",
      "1 2000-01-01  64.875 -103.875           26.860878             67.51416   \n",
      "2 2000-01-01  64.875 -104.125           26.629623             68.30016   \n",
      "3 2000-01-01  64.875 -104.375           26.186129             69.08916   \n",
      "4 2000-01-01  64.875 -104.625           25.803741             69.82496   \n",
      "\n",
      "   SoilMoi40_100cm_inst  SoilMoi100_200cm_inst  waterstorage  \n",
      "0             336.96610              456.84467    355.720494  \n",
      "1              86.47700              300.45468    187.640608  \n",
      "2              85.29900              306.18268    190.257545  \n",
      "3              85.54314              309.37466    192.022952  \n",
      "4              89.45014              309.05768    193.127813  \n",
      "\n",
      "Converting DataFrame to Xarray Dataset...\n",
      "Xarray Dataset created successfully:\n",
      "<xarray.Dataset> Size: 718MB\n",
      "Dimensions:                (time: 300, lat: 168, lon: 356)\n",
      "Coordinates:\n",
      "  * time                   (time) datetime64[ns] 2kB 2000-01-01 ... 2024-12-01\n",
      "  * lat                    (lat) float64 1kB 41.12 41.38 41.62 ... 82.62 82.88\n",
      "  * lon                    (lon) float64 3kB -140.9 -140.6 ... -52.38 -52.12\n",
      "Data variables:\n",
      "    SoilMoi0_10cm_inst     (time, lat, lon) float64 144MB nan nan ... nan nan\n",
      "    SoilMoi10_40cm_inst    (time, lat, lon) float64 144MB nan nan ... nan nan\n",
      "    SoilMoi40_100cm_inst   (time, lat, lon) float64 144MB nan nan ... nan nan\n",
      "    SoilMoi100_200cm_inst  (time, lat, lon) float64 144MB nan nan ... nan nan\n",
      "    waterstorage           (time, lat, lon) float64 144MB nan nan ... nan nan\n",
      "\n",
      "Aggregating data to 5.0° x 5.0° grid using groupby_bins...\n",
      "Latitude bins: [40. 45. 50. 55. 60. 65. 70. 75. 80. 85.]\n",
      "Latitude centroids: [42.5 47.5 52.5 57.5 62.5 67.5 72.5 77.5 82.5]\n",
      "Longitude bins: [-145. -140. -135. -130. -125. -120. -115. -110. -105. -100.  -95.  -90.\n",
      "  -85.  -80.  -75.  -70.  -65.  -60.  -55.  -50.]\n",
      "Longitude centroids: [-142.5 -137.5 -132.5 -127.5 -122.5 -117.5 -112.5 -107.5 -102.5  -97.5\n",
      "  -92.5  -87.5  -82.5  -77.5  -72.5  -67.5  -62.5  -57.5  -52.5]\n",
      "Aggregation complete.\n",
      "Aggregated Dataset:\n",
      "<xarray.Dataset> Size: 2MB\n",
      "Dimensions:                (lon: 19, lat: 9, time: 300)\n",
      "Coordinates:\n",
      "  * time                   (time) datetime64[ns] 2kB 2000-01-01 ... 2024-12-01\n",
      "  * lat                    (lat) float64 72B 42.5 47.5 52.5 ... 72.5 77.5 82.5\n",
      "  * lon                    (lon) float64 152B -142.5 -137.5 ... -57.5 -52.5\n",
      "Data variables:\n",
      "    SoilMoi0_10cm_inst     (lon, lat, time) float64 410kB nan nan ... 80.92\n",
      "    SoilMoi10_40cm_inst    (lon, lat, time) float64 410kB nan nan ... 259.1\n",
      "    SoilMoi40_100cm_inst   (lon, lat, time) float64 410kB nan nan ... 512.5\n",
      "    SoilMoi100_200cm_inst  (lon, lat, time) float64 410kB nan nan ... 826.3\n",
      "    waterstorage           (lon, lat, time) float64 410kB nan nan ... 609.8\n",
      "\n",
      "Converting aggregated Dataset back to DataFrame...\n",
      "Dropping 9900 rows with NaN 'waterstorage' values (empty grid cells).\n",
      "\n",
      "Saving aggregated data...\n",
      "Saving to CSV: ../../output/downsampled_GLDAS_5deg_water_aligned.csv\n",
      "Saving to Parquet: ../../output/downsampled_GLDAS_5deg_water_aligned.parquet\n",
      "Aggregated data saved successfully.\n",
      "\n",
      "Final DataFrame snippet:\n",
      "       lon   lat       time  SoilMoi0_10cm_inst  SoilMoi10_40cm_inst  \\\n",
      "900 -142.5  57.5 2000-01-01           83.875445           254.035585   \n",
      "901 -142.5  57.5 2000-02-01           83.731920           254.116095   \n",
      "902 -142.5  57.5 2000-03-01           81.630962           254.100155   \n",
      "903 -142.5  57.5 2000-04-01           80.474056           249.098585   \n",
      "904 -142.5  57.5 2000-05-01           79.682159           240.479580   \n",
      "\n",
      "     SoilMoi40_100cm_inst  SoilMoi100_200cm_inst  waterstorage  \n",
      "900            486.607762             847.055927    611.809402  \n",
      "901            502.109110             847.055927    616.464707  \n",
      "902            506.933877             847.055927    617.804698  \n",
      "903            508.131220             847.055927    617.355820  \n",
      "904            497.724938             847.055927    612.901490  \n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os # Added for output directory creation\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assuming df is your loaded DataFrame before this script runs\n",
    "# Example:\n",
    "# df = pd.read_csv('your_input_file.csv')\n",
    "\n",
    "output_dir = '../../output' # Define output directory\n",
    "output_csv_file = os.path.join(output_dir, 'downsampled_GLDAS_5deg_water_aligned.csv')\n",
    "output_parquet_file = os.path.join(output_dir, 'downsampled_GLDAS_5deg_water_aligned.parquet')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the input grid resolution (used for consistency check, not directly in groupby_bins)\n",
    "lat_res_in = 0.25\n",
    "lon_res_in = 0.25\n",
    "# Define the desired output grid resolution\n",
    "lat_res_out = 5.0 # Use float for calculations\n",
    "lon_res_out = 5.0 # Use float for calculations\n",
    "df = df_gldas\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Starting script...\")\n",
    "print(f\"Output CSV: {output_csv_file}\")\n",
    "print(f\"Output Parquet: {output_parquet_file}\")\n",
    "print(f\"Input resolution: {lat_res_in}° x {lon_res_in}° (assumed)\")\n",
    "print(f\"Output resolution: {lat_res_out}° x {lon_res_out}°\")\n",
    "\n",
    "# --- Step 1: Read the CSV data using Pandas ---\n",
    "# Assume 'df' is already loaded and contains 'time', 'lat', 'lon', 'waterstorage'\n",
    "print(f\"\\nUsing pre-loaded DataFrame...\")\n",
    "try:\n",
    "    # Make sure columns exist before proceeding\n",
    "    required_cols = ['time', 'lat', 'lon', 'waterstorage']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Optional: Convert 'time' column to datetime objects if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        print(\"Converting 'time' column to datetime...\")\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    print(\"DataFrame ready.\")\n",
    "    print(\"DataFrame head:\")\n",
    "    print(df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"Error: DataFrame 'df' not found.\")\n",
    "    print(\"Please ensure the DataFrame is loaded before running this script.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    exit()\n",
    "except MemoryError: # Keep memory error handling\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"MemoryError: The DataFrame is too large.\")\n",
    "    print(\"Consider using Dask or chunking if conversion fails later.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit() # Exit the script if memory error occurs\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn unexpected error occurred during DataFrame preparation: {e}\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Step 2: Convert Pandas DataFrame to Xarray Dataset ---\n",
    "print(\"\\nConverting DataFrame to Xarray Dataset...\")\n",
    "try:\n",
    "    # Check for duplicate indices *before* setting index\n",
    "    duplicates = df.duplicated(subset=['time', 'lat', 'lon']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\nWarning: Found {duplicates} duplicate time/lat/lon combinations. Keeping first occurrence.\")\n",
    "        # Optionally, decide how to handle duplicates (e.g., mean, keep first/last)\n",
    "        # df = df.groupby(['time', 'lat', 'lon']).mean().reset_index() # Example: average duplicates\n",
    "        df = df.drop_duplicates(subset=['time', 'lat', 'lon'], keep='first')\n",
    "\n",
    "    df = df.set_index(['time', 'lat', 'lon'])\n",
    "    ds = df.to_xarray()\n",
    "\n",
    "    # Ensure latitude coordinates are sorted (ascending or descending)\n",
    "    # groupby_bins requires monotonic coordinates\n",
    "    if not ds.indexes['lat'].is_monotonic_increasing and not ds.indexes['lat'].is_monotonic_decreasing:\n",
    "         print(\"Sorting latitude coordinates...\")\n",
    "         ds = ds.sortby('lat')\n",
    "    if not ds.indexes['lon'].is_monotonic_increasing and not ds.indexes['lon'].is_monotonic_decreasing:\n",
    "         print(\"Sorting longitude coordinates...\")\n",
    "         ds = ds.sortby('lon')\n",
    "\n",
    "    # Optional: Reindex latitude to be decreasing if needed (common standard)\n",
    "    # if ds['lat'][0] < ds['lat'][-1]:\n",
    "    #     print(\"Reindexing latitude to decreasing order...\")\n",
    "    #     ds = ds.reindex(lat=list(reversed(ds['lat'])))\n",
    "\n",
    "    print(\"Xarray Dataset created successfully:\")\n",
    "    print(ds)\n",
    "except KeyError as e:\n",
    "    print(f\"\\nError: Column '{e}' not found in DataFrame. Needed for setting index.\")\n",
    "    print(\"Please ensure your DataFrame has 'time', 'lat', 'lon' columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during DataFrame to Xarray conversion: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 3: Perform Spatial Aggregation using groupby_bins ---\n",
    "print(f\"\\nAggregating data to {lat_res_out}° x {lon_res_out}° grid using groupby_bins...\")\n",
    "\n",
    "try:\n",
    "    # 3a: Determine the overall extent and create output grid *boundaries*\n",
    "    min_lat, max_lat = ds['lat'].min().item(), ds['lat'].max().item()\n",
    "    min_lon, max_lon = ds['lon'].min().item(), ds['lon'].max().item()\n",
    "\n",
    "    # Calculate the boundaries aligned with the output resolution\n",
    "    # Use floor for min edge, ceil for max edge, ensuring they align with the grid\n",
    "    # Add a small epsilon to max extent before ceil to handle points exactly on boundary\n",
    "    epsilon = 1e-9\n",
    "    lat_min_bnd = np.floor(min_lat / lat_res_out) * lat_res_out\n",
    "    lat_max_bnd = np.ceil((max_lat + epsilon) / lat_res_out) * lat_res_out\n",
    "    lon_min_bnd = np.floor(min_lon / lon_res_out) * lon_res_out\n",
    "    lon_max_bnd = np.ceil((max_lon + epsilon) / lon_res_out) * lon_res_out\n",
    "\n",
    "    # Create the bin edges for grouping\n",
    "    # Use arange up to max_bnd + resolution/2 to ensure the last bin edge is included\n",
    "    lat_bins = np.arange(lat_min_bnd, lat_max_bnd + lat_res_out / 2, lat_res_out)\n",
    "    lon_bins = np.arange(lon_min_bnd, lon_max_bnd + lon_res_out / 2, lon_res_out)\n",
    "\n",
    "    # 3b: Calculate the desired output grid *centroids* (labels for the bins)\n",
    "    # These will become the new coordinates in the aggregated dataset\n",
    "    lat_centroids = lat_bins[:-1] + lat_res_out / 2.0\n",
    "    lon_centroids = lon_bins[:-1] + lon_res_out / 2.0\n",
    "\n",
    "    print(f\"Latitude bins: {lat_bins}\")\n",
    "    print(f\"Latitude centroids: {lat_centroids}\")\n",
    "    print(f\"Longitude bins: {lon_bins}\")\n",
    "    print(f\"Longitude centroids: {lon_centroids}\")\n",
    "\n",
    "    # 3c: Perform the aggregation\n",
    "    # Group by latitude bins first, calculate the mean for each bin,\n",
    "    # then group the result by longitude bins and calculate the mean.\n",
    "    # Use the calculated centroids as the coordinate labels for the resulting bins.\n",
    "    # 'right=False' means bins are [left, right), matching np.floor logic.\n",
    "    ds_agg = ds.groupby_bins('lat', bins=lat_bins, labels=lat_centroids, right=False).mean()\n",
    "    ds_agg = ds_agg.groupby_bins('lon', bins=lon_bins, labels=lon_centroids, right=False).mean()\n",
    "\n",
    "    # Rename the coordinate dimensions from 'lat_bins'/'lon_bins' back to 'lat'/'lon'\n",
    "    ds_agg = ds_agg.rename({'lat_bins': 'lat', 'lon_bins': 'lon'})\n",
    "   \n",
    "\n",
    "    print(\"Aggregation complete.\")\n",
    "    print(\"Aggregated Dataset:\")\n",
    "    print(ds_agg)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during aggregation with groupby_bins: {e}\")\n",
    "    print(\"Check if latitude/longitude coordinates are monotonic (sorted).\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 4: Convert back to DataFrame and save ---\n",
    "print(\"\\nConverting aggregated Dataset back to DataFrame...\")\n",
    "# Use .reset_index() to turn the coordinates ('time', 'lat', 'lon')\n",
    "# back into columns for the CSV/Parquet output.\n",
    "df_agg = ds_agg.to_dataframe().reset_index()\n",
    "\n",
    "# Optional: Drop rows where the aggregated value is NaN\n",
    "# This happens if an output grid cell contained no input data points.\n",
    "nan_count_before = df_agg['waterstorage'].isnull().sum()\n",
    "if nan_count_before > 0:\n",
    "    print(f\"Dropping {nan_count_before} rows with NaN 'waterstorage' values (empty grid cells).\")\n",
    "    df_agg = df_agg.dropna(subset=['waterstorage'])\n",
    "\n",
    "print(f\"\\nSaving aggregated data...\")\n",
    "try:\n",
    "    # Save to CSV\n",
    "    print(f\"Saving to CSV: {output_csv_file}\")\n",
    "    df_agg.to_csv(output_csv_file, index=False, float_format='%.5f') # Control float precision\n",
    "\n",
    "    # Save to Parquet (often more efficient for large data)\n",
    "    print(f\"Saving to Parquet: {output_parquet_file}\")\n",
    "    df_agg.to_parquet(output_parquet_file, index=False)\n",
    "\n",
    "    df_gldas = df_agg.copy()\n",
    "\n",
    "    print(\"Aggregated data saved successfully.\")\n",
    "    print(\"\\nFinal DataFrame snippet:\")\n",
    "    print(df_agg.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the output files: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776525d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lon   lat       time  waterstorage_grace\n",
      "0 -127.5  47.5 2002-04-01        0.000000e+00\n",
      "1 -127.5  47.5 2002-05-01        4.336809e-19\n",
      "2 -127.5  47.5 2002-06-01       -2.602085e-18\n",
      "3 -127.5  47.5 2002-07-01       -1.040834e-17\n",
      "4 -127.5  47.5 2002-08-01        3.469447e-18\n",
      "       lon   lat       time  SoilMoi0_10cm_inst  SoilMoi10_40cm_inst  \\\n",
      "900 -142.5  57.5 2000-01-01           83.875445           254.035585   \n",
      "901 -142.5  57.5 2000-02-01           83.731920           254.116095   \n",
      "902 -142.5  57.5 2000-03-01           81.630962           254.100155   \n",
      "903 -142.5  57.5 2000-04-01           80.474056           249.098585   \n",
      "904 -142.5  57.5 2000-05-01           79.682159           240.479580   \n",
      "\n",
      "     SoilMoi40_100cm_inst  SoilMoi100_200cm_inst  waterstorage_gldas  \n",
      "900            486.607762             847.055927          611.809402  \n",
      "901            502.109110             847.055927          616.464707  \n",
      "902            506.933877             847.055927          617.804698  \n",
      "903            508.131220             847.055927          617.355820  \n",
      "904            497.724938             847.055927          612.901490  \n",
      "     lon   lat       time  waterstorage_grace  waterstorage_gldas\n",
      "0 -127.5  47.5 2002-04-01        0.000000e+00          223.875575\n",
      "1 -127.5  47.5 2002-05-01        4.336809e-19          199.540413\n",
      "2 -127.5  47.5 2002-06-01       -2.602085e-18          188.398153\n",
      "3 -127.5  47.5 2002-07-01       -1.040834e-17          178.760345\n",
      "4 -127.5  47.5 2002-08-01        3.469447e-18          149.738318\n",
      "Merged and Standardized Data:\n",
      "     lon   lat       time  waterstorage_grace  waterstorage_gldas  grace_std  \\\n",
      "0 -127.5  47.5 2002-04-01        0.000000e+00          223.875575   0.216768   \n",
      "1 -127.5  47.5 2002-05-01        4.336809e-19          199.540413   0.216768   \n",
      "2 -127.5  47.5 2002-06-01       -2.602085e-18          188.398153   0.216768   \n",
      "3 -127.5  47.5 2002-07-01       -1.040834e-17          178.760345   0.216768   \n",
      "4 -127.5  47.5 2002-08-01        3.469447e-18          149.738318   0.216768   \n",
      "\n",
      "   gldas_std  waterstorage  \n",
      "0   0.333396      0.275082  \n",
      "1  -0.063647      0.076561  \n",
      "2  -0.245439     -0.014336  \n",
      "3  -0.402686     -0.092959  \n",
      "4  -0.876197     -0.329715  \n",
      "Merged data saved to CSV: ../../output/merged_standardized_data_0.5_0.5.csv\n",
      "Merged data saved to Parquet: ../../output/merged_standardized_data_0.5_0.5.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Prepare Data ---\n",
    "# Convert time columns to datetime objects\n",
    "df_grace['time'] = pd.to_datetime(df_grace['time'])\n",
    "df_gldas['time'] = pd.to_datetime(df_gldas['time'])\n",
    "\n",
    "# Rename waterstorage columns for clarity after merge\n",
    "df_grace = df_grace.rename(columns={'waterstorage': 'waterstorage_grace'})\n",
    "df_gldas = df_gldas.rename(columns={'waterstorage': 'waterstorage_gldas'})\n",
    "print(df_grace.head())\n",
    "print(df_gldas.head())\n",
    "\n",
    "# Select relevant columns from GLDAS (optional, keeps df smaller)\n",
    "df_gldas_subset = df_gldas[['lon', 'lat', 'time', 'waterstorage_gldas']]\n",
    "\n",
    "# --- 3. Merge ---\n",
    "# Merge based on location and time\n",
    "# Use 'inner' merge to only keep rows where lon, lat, and time match in both datasets\n",
    "df_merged = pd.merge(df_grace, df_gldas_subset, on=['lon', 'lat', 'time'], how='inner')\n",
    "print(df_merged.head())\n",
    "# --- 4. Standardize ---\n",
    "# Calculate mean and standard deviation for each waterstorage column *within the merged data*\n",
    "grace_mean = df_merged['waterstorage_grace'].mean()\n",
    "grace_std = df_merged['waterstorage_grace'].std()\n",
    "\n",
    "gldas_mean = df_merged['waterstorage_gldas'].mean()\n",
    "gldas_std = df_merged['waterstorage_gldas'].std()\n",
    "\n",
    "# Add a small epsilon to standard deviation to prevent division by zero if std is 0\n",
    "epsilon = 1e-9 \n",
    "\n",
    "# Calculate standardized (Z-score) values\n",
    "df_merged['grace_std'] = (df_merged['waterstorage_grace'] - grace_mean) / (grace_std + epsilon)\n",
    "df_merged['gldas_std'] = (df_merged['waterstorage_gldas'] - gldas_mean) / (gldas_std + epsilon)\n",
    "\n",
    "# --- 5. Combine ---\n",
    "# Calculate the combined standardized value\n",
    "df_merged['combined_std'] = 0.5 * df_merged['grace_std'] + 0.5 * df_merged['gldas_std']\n",
    "df_merged.rename(columns={'combined_std': 'waterstorage'}, inplace=True)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"Merged and Standardized Data:\")\n",
    "# Show relevant columns\n",
    "print(df_merged[['lon', 'lat', 'time', 'waterstorage_grace', 'waterstorage_gldas', 'grace_std', 'gldas_std', 'waterstorage']].head())\n",
    "# Save the merged DataFrame to CSV\n",
    "output_merged_csv_file = os.path.join(output_dir, 'merged_standardized_data_0.5_0.5.csv')\n",
    "df_merged.to_csv(output_merged_csv_file, index=False, float_format='%.5f') # Control float precision\n",
    "# Save the merged DataFrame to Parquet\n",
    "output_merged_parquet_file = os.path.join(output_dir, 'merged_standardized_data_0.5_0.5.parquet')\n",
    "df_merged.to_parquet(output_merged_parquet_file, index=False)\n",
    "print(f\"Merged data saved to CSV: {output_merged_csv_file}\")\n",
    "print(f\"Merged data saved to Parquet: {output_merged_parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd2132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
