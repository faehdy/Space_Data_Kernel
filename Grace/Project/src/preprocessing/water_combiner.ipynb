{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0639ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b12ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/data_GLDAS/compiled_canada_soil_moisture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93928fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'lat', 'lon', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst',\n",
       "       'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd31bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'], axis = 1)\n",
    "# rename the column 'SoilMoi0_10cm_inst' to 'waterstorage'\n",
    "df = df.rename(columns={'SoilMoi0_10cm_inst': 'waterstorage'})\n",
    "# gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']))\n",
    "# gdf = gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# gdf.to_parquet('../Data/data_GLDAS/gdf_compiled_canada_soil_moisture.parquet', index=False)\n",
    "\n",
    "#create a simple df with 4 entries which build a coordinate grid with 0.25° resolution\n",
    "# df_test = pd.DataFrame(columns=['lon', 'lat', 'waterstorage'])\n",
    "# lon = []\n",
    "# lat = []\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         lon_ = 100.125 + i * 0.25\n",
    "#         lat_ = 10.125 + j * 0.25\n",
    "#         lon.append(lon_)\n",
    "#         lat.append(lat_)\n",
    "# # list_lon\n",
    "# df_test = pd.DataFrame({\n",
    "#     'lon': lon,\n",
    "#     'lat': lat,\n",
    "#     'waterstorage': np.random.rand(16),\n",
    "#     'time': pd.datetime(2023, 1, 1) \n",
    "# })\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7f814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Output file: ../../output/downsampled_1deg_water.csv\n",
      "Input resolution: 0.25° x 0.25°\n",
      "Output resolution: 1.0° x 1.0°\n",
      "\n",
      "Reading input file...\n",
      "CSV read complete.\n",
      "DataFrame head:\n",
      "         time     lat      lon  waterstorage\n",
      "0  2000-01-01  82.875  -67.625     32.961132\n",
      "1  2000-01-01  64.875 -103.875     26.860878\n",
      "2  2000-01-01  64.875 -104.125     26.629623\n",
      "3  2000-01-01  64.875 -104.375     26.186129\n",
      "4  2000-01-01  64.875 -104.625     25.803741\n",
      "\n",
      "Converting DataFrame to Xarray Dataset...\n",
      "Xarray Dataset created successfully:\n",
      "<xarray.Dataset> Size: 144MB\n",
      "Dimensions:       (time: 300, lat: 168, lon: 356)\n",
      "Coordinates:\n",
      "  * time          (time) object 2kB '2000-01-01' '2000-02-01' ... '2024-12-01'\n",
      "  * lat           (lat) float64 1kB 41.12 41.38 41.62 ... 82.38 82.62 82.88\n",
      "  * lon           (lon) float64 3kB -140.9 -140.6 -140.4 ... -52.38 -52.12\n",
      "Data variables:\n",
      "    waterstorage  (time, lat, lon) float64 144MB nan nan nan nan ... nan nan nan\n",
      "\n",
      "Coarsening data with latitude window=4, longitude window=4...\n",
      "Coarsening complete.\n",
      "Downsampled Dataset:\n",
      "<xarray.Dataset> Size: 9MB\n",
      "Dimensions:       (time: 300, lat: 42, lon: 89)\n",
      "Coordinates:\n",
      "  * time          (time) object 2kB '2000-01-01' '2000-02-01' ... '2024-12-01'\n",
      "  * lat           (lat) float64 336B 41.5 42.5 43.5 44.5 ... 79.5 80.5 81.5 82.5\n",
      "  * lon           (lon) float64 712B -140.5 -139.5 -138.5 ... -54.5 -53.5 -52.5\n",
      "Data variables:\n",
      "    waterstorage  (time, lat, lon) float64 9MB nan nan nan nan ... nan 88.68 nan\n",
      "\n",
      "Converting downsampled Dataset back to DataFrame...\n",
      "Saving downsampled data to ../../output/downsampled_1deg_water.csv...\n",
      "Downsampled data saved successfully.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace 'your_large_file.csv' with the actual path to your file\n",
    "\n",
    "\n",
    "output_csv_file = '../../output/downsampled_1deg_water.csv'\n",
    "\n",
    "# Define the input and output grid resolutions\n",
    "lat_res_in = 0.25\n",
    "lon_res_in = 0.25\n",
    "lat_res_out = 1.0\n",
    "lon_res_out = 1.0\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Starting script...\")\n",
    "print(f\"Output file: {output_csv_file}\")\n",
    "print(f\"Input resolution: {lat_res_in}° x {lon_res_in}°\")\n",
    "print(f\"Output resolution: {lat_res_out}° x {lon_res_out}°\")\n",
    "\n",
    "# --- Step 1: Read the CSV data using Pandas ---\n",
    "print(f\"\\nReading input file...\")\n",
    "# For very large files (like 600MB), this might consume a lot of memory.\n",
    "# If you encounter MemoryError, consider:\n",
    "# 1. Using Dask DataFrames: `import dask.dataframe as dd; df = dd.read_csv(...)`\n",
    "#    which reads and processes data in chunks.\n",
    "# 2. Using Pandas chunking: Iterate through chunks of the CSV\n",
    "#    `chunks = pd.read_csv(input_csv_file, chunksize=100000)` and process chunk by chunk.\n",
    "# This example assumes the file fits into memory for simplicity.\n",
    "try:\n",
    "    df = df\n",
    "    # Optional: Convert 'time' column to datetime objects if it's not already\n",
    "    # df['time'] = pd.to_datetime(df['time'])\n",
    "    print(\"CSV read complete.\")\n",
    "    print(\"DataFrame head:\")\n",
    "    print(df.head())\n",
    "except MemoryError:\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"MemoryError: The file is too large to read directly into memory with Pandas.\")\n",
    "    print(\"Consider using Dask DataFrames (`dask.dataframe`) or \")\n",
    "    print(\"reading the file in chunks with `pd.read_csv(..., chunksize=...)`.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit() # Exit the script if memory error occurs\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Input file not found   \")\n",
    "    print(\"Please ensure the file path is correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Convert Pandas DataFrame to Xarray Dataset ---\n",
    "# Xarray works best with multi-dimensional data. We need to set\n",
    "# 'time', 'lat', 'lon' as index dimensions for the conversion.\n",
    "print(\"\\nConverting DataFrame to Xarray Dataset...\")\n",
    "try:\n",
    "    df = df.set_index(['time', 'lat', 'lon'])\n",
    "    ds = df.to_xarray()\n",
    "    # Ensure latitude is decreasing if needed (common in climate data)\n",
    "    # if ds['lat'][0] < ds['lat'][-1]:\n",
    "    #     ds = ds.reindex(lat=list(reversed(ds['lat'])))\n",
    "    print(\"Xarray Dataset created successfully:\")\n",
    "    print(ds)\n",
    "except KeyError as e:\n",
    "     print(f\"\\nError: Column '{e}' not found in CSV. Needed for setting index.\")\n",
    "     print(\"Please ensure your CSV has 'time', 'lat', 'lon' columns.\")\n",
    "     exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during DataFrame to Xarray conversion: {e}\")\n",
    "    print(\"This might happen if there are duplicate time/lat/lon combinations.\")\n",
    "    # Optional: Add code here to check for and handle duplicates if necessary\n",
    "    # print(\"Checking for duplicate index entries...\")\n",
    "    # duplicates = df.index.duplicated(keep=False)\n",
    "    # print(f\"Found {duplicates.sum()} duplicate index entries.\")\n",
    "    # print(\"Example duplicates:\")\n",
    "    # print(df[duplicates].head())\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 3: Perform the spatial downsampling (coarsening) ---\n",
    "# Calculate the window size needed to go from 0.25° to 1°\n",
    "# Ensure integer division or handle potential floating point issues if resolutions were different\n",
    "lat_window = int(lat_res_out / lat_res_in)\n",
    "lon_window = int(lon_res_out / lon_res_in)\n",
    "\n",
    "if lat_window <= 0 or lon_window <= 0:\n",
    "    print(\"\\nError: Output resolution must be larger than input resolution for coarsening.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nCoarsening data with latitude window={lat_window}, longitude window={lon_window}...\")\n",
    "\n",
    "# Use the coarsen method. It selects blocks of size {lat: lat_window, lon: lon_window}\n",
    "# and applies the function (in this case, mean) to each block.\n",
    "# 'boundary=\"pad\"' handles edges where the dimensions might not be perfectly\n",
    "# divisible by the window size (shouldn't be an issue for 0.25 -> 1).\n",
    "try:\n",
    "    ds_coarse = ds.coarsen(lat=lat_window, lon=lon_window, boundary='pad').mean()\n",
    "    print(\"Coarsening complete.\")\n",
    "    print(\"Downsampled Dataset:\")\n",
    "    print(ds_coarse)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during coarsening: {e}\")\n",
    "    print(\"Check if latitude/longitude coordinates are regularly spaced.\")\n",
    "    exit()\n",
    "\n",
    "# Note on coordinates: The coordinates of the coarse grid ('lat', 'lon')\n",
    "# usually represent the center/start of the *first* cell in each window by default.\n",
    "# If you need coordinates representing the center of the new 1°x1° cell,\n",
    "# you might need to calculate and assign them manually after coarsening.\n",
    "# Example:\n",
    "# new_lat = ds_coarse['lat'] + (lat_res_out / 2.0) - (lat_res_in / 2.0)\n",
    "# new_lon = ds_coarse['lon'] + (lon_res_out / 2.0) - (lon_res_in / 2.0)\n",
    "# ds_coarse = ds_coarse.assign_coords(lat=new_lat, lon=new_lon)\n",
    "\n",
    "\n",
    "# --- Step 4: Convert back to DataFrame and save to CSV ---\n",
    "print(\"\\nConverting downsampled Dataset back to DataFrame...\")\n",
    "# Use .reset_index() to turn the coordinates ('time', 'lat', 'lon')\n",
    "# back into columns for the CSV output.\n",
    "df_coarse = ds_coarse.to_dataframe().reset_index()\n",
    "\n",
    "#drop the nan rows\n",
    "df_coarse = df_coarse.dropna(subset=['waterstorage'])\n",
    "\n",
    "\n",
    "print(f\"Saving downsampled data to {output_csv_file}...\")\n",
    "try:\n",
    "    df_coarse.to_csv(output_csv_file, index=False, float_format='%.5f') # Control float precision\n",
    "    df_coarse.to_parquet('../../output/downsampled_1deg_water.parquet', index=False)\n",
    "    # print(df_coarse)\n",
    "    print(\"Downsampled data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the CSV: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43ec88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
