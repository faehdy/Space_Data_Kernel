{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0639ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b12ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/data_GLDAS/compiled_canada_soil_moisture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93928fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'lat', 'lon', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst',\n",
       "       'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd31bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst'], axis = 1)\n",
    "# rename the column 'SoilMoi0_10cm_inst' to 'waterstorage'\n",
    "df = df.rename(columns={'SoilMoi0_10cm_inst': 'waterstorage'})\n",
    "# gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']))\n",
    "# gdf = gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# gdf.to_parquet('../Data/data_GLDAS/gdf_compiled_canada_soil_moisture.parquet', index=False)\n",
    "\n",
    "#create a simple df with 4 entries which build a coordinate grid with 0.25° resolution\n",
    "# df_test = pd.DataFrame(columns=['lon', 'lat', 'waterstorage'])\n",
    "# lon = []\n",
    "# lat = []\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         lon_ = 100.125 + i * 0.25\n",
    "#         lat_ = 10.125 + j * 0.25\n",
    "#         lon.append(lon_)\n",
    "#         lat.append(lat_)\n",
    "# # list_lon\n",
    "# df_test = pd.DataFrame({\n",
    "#     'lon': lon,\n",
    "#     'lat': lat,\n",
    "#     'waterstorage': np.random.rand(16),\n",
    "#     'time': pd.datetime(2023, 1, 1) \n",
    "# })\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os # Added for output directory creation\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assuming df is your loaded DataFrame before this script runs\n",
    "# Example:\n",
    "# df = pd.read_csv('your_input_file.csv')\n",
    "\n",
    "output_dir = '../../output' # Define output directory\n",
    "output_csv_file = os.path.join(output_dir, 'downsampled_5deg_water_aligned.csv')\n",
    "output_parquet_file = os.path.join(output_dir, 'downsampled_5deg_water_aligned.parquet')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the input grid resolution (used for consistency check, not directly in groupby_bins)\n",
    "lat_res_in = 0.25\n",
    "lon_res_in = 0.25\n",
    "# Define the desired output grid resolution\n",
    "lat_res_out = 5.0 # Use float for calculations\n",
    "lon_res_out = 5.0 # Use float for calculations\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Starting script...\")\n",
    "print(f\"Output CSV: {output_csv_file}\")\n",
    "print(f\"Output Parquet: {output_parquet_file}\")\n",
    "print(f\"Input resolution: {lat_res_in}° x {lon_res_in}° (assumed)\")\n",
    "print(f\"Output resolution: {lat_res_out}° x {lon_res_out}°\")\n",
    "\n",
    "# --- Step 1: Read the CSV data using Pandas ---\n",
    "# Assume 'df' is already loaded and contains 'time', 'lat', 'lon', 'waterstorage'\n",
    "print(f\"\\nUsing pre-loaded DataFrame...\")\n",
    "try:\n",
    "    # Make sure columns exist before proceeding\n",
    "    required_cols = ['time', 'lat', 'lon', 'waterstorage']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Optional: Convert 'time' column to datetime objects if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        print(\"Converting 'time' column to datetime...\")\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    print(\"DataFrame ready.\")\n",
    "    print(\"DataFrame head:\")\n",
    "    print(df.head())\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"Error: DataFrame 'df' not found.\")\n",
    "    print(\"Please ensure the DataFrame is loaded before running this script.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    exit()\n",
    "except MemoryError: # Keep memory error handling\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    print(\"MemoryError: The DataFrame is too large.\")\n",
    "    print(\"Consider using Dask or chunking if conversion fails later.\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    exit() # Exit the script if memory error occurs\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn unexpected error occurred during DataFrame preparation: {e}\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Step 2: Convert Pandas DataFrame to Xarray Dataset ---\n",
    "print(\"\\nConverting DataFrame to Xarray Dataset...\")\n",
    "try:\n",
    "    # Check for duplicate indices *before* setting index\n",
    "    duplicates = df.duplicated(subset=['time', 'lat', 'lon']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\nWarning: Found {duplicates} duplicate time/lat/lon combinations. Keeping first occurrence.\")\n",
    "        # Optionally, decide how to handle duplicates (e.g., mean, keep first/last)\n",
    "        # df = df.groupby(['time', 'lat', 'lon']).mean().reset_index() # Example: average duplicates\n",
    "        df = df.drop_duplicates(subset=['time', 'lat', 'lon'], keep='first')\n",
    "\n",
    "    df = df.set_index(['time', 'lat', 'lon'])\n",
    "    ds = df.to_xarray()\n",
    "\n",
    "    # Ensure latitude coordinates are sorted (ascending or descending)\n",
    "    # groupby_bins requires monotonic coordinates\n",
    "    if not ds.indexes['lat'].is_monotonic_increasing and not ds.indexes['lat'].is_monotonic_decreasing:\n",
    "         print(\"Sorting latitude coordinates...\")\n",
    "         ds = ds.sortby('lat')\n",
    "    if not ds.indexes['lon'].is_monotonic_increasing and not ds.indexes['lon'].is_monotonic_decreasing:\n",
    "         print(\"Sorting longitude coordinates...\")\n",
    "         ds = ds.sortby('lon')\n",
    "\n",
    "    # Optional: Reindex latitude to be decreasing if needed (common standard)\n",
    "    # if ds['lat'][0] < ds['lat'][-1]:\n",
    "    #     print(\"Reindexing latitude to decreasing order...\")\n",
    "    #     ds = ds.reindex(lat=list(reversed(ds['lat'])))\n",
    "\n",
    "    print(\"Xarray Dataset created successfully:\")\n",
    "    print(ds)\n",
    "except KeyError as e:\n",
    "    print(f\"\\nError: Column '{e}' not found in DataFrame. Needed for setting index.\")\n",
    "    print(\"Please ensure your DataFrame has 'time', 'lat', 'lon' columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during DataFrame to Xarray conversion: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 3: Perform Spatial Aggregation using groupby_bins ---\n",
    "print(f\"\\nAggregating data to {lat_res_out}° x {lon_res_out}° grid using groupby_bins...\")\n",
    "\n",
    "try:\n",
    "    # 3a: Determine the overall extent and create output grid *boundaries*\n",
    "    min_lat, max_lat = ds['lat'].min().item(), ds['lat'].max().item()\n",
    "    min_lon, max_lon = ds['lon'].min().item(), ds['lon'].max().item()\n",
    "\n",
    "    # Calculate the boundaries aligned with the output resolution\n",
    "    # Use floor for min edge, ceil for max edge, ensuring they align with the grid\n",
    "    # Add a small epsilon to max extent before ceil to handle points exactly on boundary\n",
    "    epsilon = 1e-9\n",
    "    lat_min_bnd = np.floor(min_lat / lat_res_out) * lat_res_out\n",
    "    lat_max_bnd = np.ceil((max_lat + epsilon) / lat_res_out) * lat_res_out\n",
    "    lon_min_bnd = np.floor(min_lon / lon_res_out) * lon_res_out\n",
    "    lon_max_bnd = np.ceil((max_lon + epsilon) / lon_res_out) * lon_res_out\n",
    "\n",
    "    # Create the bin edges for grouping\n",
    "    # Use arange up to max_bnd + resolution/2 to ensure the last bin edge is included\n",
    "    lat_bins = np.arange(lat_min_bnd, lat_max_bnd + lat_res_out / 2, lat_res_out)\n",
    "    lon_bins = np.arange(lon_min_bnd, lon_max_bnd + lon_res_out / 2, lon_res_out)\n",
    "\n",
    "    # 3b: Calculate the desired output grid *centroids* (labels for the bins)\n",
    "    # These will become the new coordinates in the aggregated dataset\n",
    "    lat_centroids = lat_bins[:-1] + lat_res_out / 2.0\n",
    "    lon_centroids = lon_bins[:-1] + lon_res_out / 2.0\n",
    "\n",
    "    print(f\"Latitude bins: {lat_bins}\")\n",
    "    print(f\"Latitude centroids: {lat_centroids}\")\n",
    "    print(f\"Longitude bins: {lon_bins}\")\n",
    "    print(f\"Longitude centroids: {lon_centroids}\")\n",
    "\n",
    "    # 3c: Perform the aggregation\n",
    "    # Group by latitude bins first, calculate the mean for each bin,\n",
    "    # then group the result by longitude bins and calculate the mean.\n",
    "    # Use the calculated centroids as the coordinate labels for the resulting bins.\n",
    "    # 'right=False' means bins are [left, right), matching np.floor logic.\n",
    "    ds_agg = ds.groupby_bins('lat', bins=lat_bins, labels=lat_centroids, right=False).mean()\n",
    "    ds_agg = ds_agg.groupby_bins('lon', bins=lon_bins, labels=lon_centroids, right=False).mean()\n",
    "\n",
    "    # Rename the coordinate dimensions from 'lat_bins'/'lon_bins' back to 'lat'/'lon'\n",
    "    ds_agg = ds_agg.rename({'lat_bins': 'lat', 'lon_bins': 'lon'})\n",
    "\n",
    "    print(\"Aggregation complete.\")\n",
    "    print(\"Aggregated Dataset:\")\n",
    "    print(ds_agg)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during aggregation with groupby_bins: {e}\")\n",
    "    print(\"Check if latitude/longitude coordinates are monotonic (sorted).\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 4: Convert back to DataFrame and save ---\n",
    "print(\"\\nConverting aggregated Dataset back to DataFrame...\")\n",
    "# Use .reset_index() to turn the coordinates ('time', 'lat', 'lon')\n",
    "# back into columns for the CSV/Parquet output.\n",
    "df_agg = ds_agg.to_dataframe().reset_index()\n",
    "\n",
    "# Optional: Drop rows where the aggregated value is NaN\n",
    "# This happens if an output grid cell contained no input data points.\n",
    "nan_count_before = df_agg['waterstorage'].isnull().sum()\n",
    "if nan_count_before > 0:\n",
    "    print(f\"Dropping {nan_count_before} rows with NaN 'waterstorage' values (empty grid cells).\")\n",
    "    df_agg = df_agg.dropna(subset=['waterstorage'])\n",
    "\n",
    "print(f\"\\nSaving aggregated data...\")\n",
    "try:\n",
    "    # Save to CSV\n",
    "    print(f\"Saving to CSV: {output_csv_file}\")\n",
    "    df_agg.to_csv(output_csv_file, index=False, float_format='%.5f') # Control float precision\n",
    "\n",
    "    # Save to Parquet (often more efficient for large data)\n",
    "    print(f\"Saving to Parquet: {output_parquet_file}\")\n",
    "    df_agg.to_parquet(output_parquet_file, index=False)\n",
    "\n",
    "    print(\"Aggregated data saved successfully.\")\n",
    "    print(\"\\nFinal DataFrame snippet:\")\n",
    "    print(df_agg.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the output files: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
